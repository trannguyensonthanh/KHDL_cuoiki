# ==============================================================================
# FILE: recommender_engine.py
# CH·ª®C NƒÇNG: B·ªô n√£o x·ª≠ l√Ω ch√≠nh (Data -> Model -> Recommend -> Evaluate)
# ==============================================================================
from sklearn.metrics import mean_absolute_error
from collections import defaultdict
import os
import re
import pickle
import numpy as np
import pandas as pd
from math import sqrt
import torch.nn.functional as F
from config import GEMINI_API_KEY
# Th∆∞ vi·ªán AI & Machine Learning
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.metrics import mean_squared_error
from sklearn.metrics.pairwise import cosine_similarity
from surprise import Dataset as SurpriseDataset, Reader, SVD, dump
import json
import time
from google import genai
from google.genai import types
from datetime import datetime
# C·∫•u h√¨nh thi·∫øt b·ªã (∆Øu ti√™n GPU n·∫øu c√≥)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"‚öôÔ∏è Thi·∫øt b·ªã t√≠nh to√°n: {device}")



# ==============================================================================
# 1. X·ª¨ L√ù D·ªÆ LI·ªÜU (DATA PROCESSOR)
# ==============================================================================
class CarDataProcessor:
    def __init__(self, file_path="scraped_cars.csv"):
        self.file_path = file_path

    def process(self):
        if not os.path.exists(self.file_path):
            raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y file {self.file_path}")

        print(f"1. [Data] ƒê·ªçc d·ªØ li·ªáu t·ª´ CSV: {self.file_path}")
        try:
            df = pd.read_csv(self.file_path, encoding='utf-8-sig')
        except UnicodeDecodeError:
            df = pd.read_csv(self.file_path, encoding='utf-8')

        # ======================================================================
        # 1. CHU·∫®N H√ìA S·ªê LI·ªÜU C∆† B·∫¢N (NUMERIC CLEANING)
        # ======================================================================
        df['id'] = df['id'].astype(str) 
        # Gi√° xe (VNƒê)
        df['price'] = pd.to_numeric(df['price'], errors='coerce').fillna(0)
        
        # NƒÉm s·∫£n xu·∫•t
        current_year = datetime.now().year
        df['year'] = pd.to_numeric(df['year'], errors='coerce').fillna(current_year - 5)
        df['age'] = current_year - df['year'] # Tu·ªïi xe
        
        # M√£ l·ª±c (Power)
        df['power'] = pd.to_numeric(df['horsepower'], errors='coerce').fillna(df['horsepower'].mean())
        
        # S·ªë ch·ªó ng·ªìi
        df['n_seats'] = pd.to_numeric(df['seats'], errors='coerce').fillna(5)

        # ODO (S·ªë km ƒë√£ ƒëi) - Gi·∫£ l·∫≠p th√¥ng minh n·∫øu thi·∫øu
        # Logic: Xe l∆∞·ªõt (<2 tu·ªïi) ƒëi √≠t (10k/nƒÉm), xe c≈© ƒëi nhi·ªÅu (15k/nƒÉm)
        if 'odo' not in df.columns:
            df['mileage'] = df.apply(
                lambda x: (current_year - x['year']) * (10000 if (current_year - x['year']) < 3 else 15000), 
                axis=1
            )
        else:
            # N·∫øu c√≥ c·ªôt odo th·∫≠t th√¨ d√πng, clean ch·ªØ 'km'
            df['mileage'] = df['odo'].astype(str).str.replace(r'\D', '', regex=True)
            df['mileage'] = pd.to_numeric(df['mileage'], errors='coerce').fillna((current_year - df['year']) * 12000)

        # ======================================================================
        # 2. PH√ÇN LO·∫†I D√íNG XE TH√îNG MINH (ADVANCED BODY TYPE CLASSIFICATION)
        # ======================================================================
        def classify_car_type(row):
            text = (str(row['name']) + " " + str(row.get('description', ''))).lower()
            seats = row['n_seats']
            
            # ∆Øu ti√™n theo t·ª´ kh√≥a
            if re.search(r'b√°n t·∫£i|pickup|ranger|triton|hilux|navara|bt-50', text):
                return 'pickup'
            if re.search(r'mpv|carnival|stargazer|custin|innova|xpander|veloz|avanza|xl7|ertiga', text):
                return 'mpv'
            if re.search(r'suv|cross|g·∫ßm cao|cx-|cr-v|tucson|santafe|sorento|everest|fortuner|glc|x3|x5', text):
                return 'suv'
            if re.search(r'hatchback|yaris|swift|morning|i10|wigo|fadil|jazz', text):
                return 'hatchback'
            if re.search(r'coupe|mui tr·∫ßn|convertible|sport|2 c·ª≠a', text):
                return 'sport'
            
            # Fallback theo s·ªë ch·ªó
            if seats >= 7: return 'mpv' # 7 ch·ªó th∆∞·ªùng l√† MPV ho·∫∑c SUV (ƒë√£ l·ªçc ·ªü tr√™n) -> g√°n MPV cho ch·∫Øc
            return 'sedan' # M·∫∑c ƒë·ªãnh c√≤n l·∫°i l√† Sedan (Vios, Accent, Camry...)

        df['car_type'] = df.apply(classify_car_type, axis=1)

        # ======================================================================
        # 3. CHU·∫®N H√ìA TEXT (CATEGORY NORMALIZATION)
        # ======================================================================
        
        # H√£ng xe
        df['make'] = df['brand'].astype(str).str.strip().str.title()
        
        # Nhi√™n li·ªáu (G·ªôp nh√≥m)
        def clean_fuel(f):
            f = str(f).lower()
            if 'ƒëi·ªán' in f or 'electric' in f: return 'Electric'
            if 'hybrid' in f: return 'Hybrid'
            if 'd·∫ßu' in f or 'diesel' in f: return 'Diesel'
            return 'Petrol'
        df['fuel_category'] = df['fuelType'].apply(clean_fuel)

        # H·ªôp s·ªë
        df['is_automatic'] = df['transmission'].astype(str).apply(
            lambda x: 1 if 't·ª± ƒë·ªông' in x.lower() or 'at' in x.lower() else 0
        )

        # ======================================================================
        # 4. TR√çCH XU·∫§T T√çNH NƒÇNG CAO C·∫§P (FEATURE EXTRACTION)
        # ======================================================================
        # T·∫°o c·ªôt ƒëi·ªÉm c√¥ng ngh·ªá (Tech Score) ƒë·ªÉ ph√¢n bi·ªát b·∫£n thi·∫øu/ƒë·ªß
        
        # Danh s√°ch t·ª´ kh√≥a t√≠nh nƒÉng x·ªãn
        tech_keywords = {
            'has_sunroof': ['c·ª≠a s·ªï tr·ªùi', 'sunroof', 'panorama'],
            'has_adas': ['adas', 'gi·ªØ l√†n', 'phanh t·ª± ƒë·ªông', 'c·∫£nh b√°o va ch·∫°m', 'honda sensing', 'toyota safety sense'],
            'has_360': ['camera 360', 'cam 360'],
            'has_leather': ['gh·∫ø da', 'da nappa'],
            'has_smartkey': ['start/stop', 'kh·ªüi ƒë·ªông n√∫t b·∫•m', 'smartkey'],
            'has_cruise': ['cruise control', 'ga t·ª± ƒë·ªông']
        }

        # T·∫°o c√°c c·ªôt flag (0/1)
        full_text = (df['features'].fillna('') + " " + df['description'].fillna('')).str.lower()
        
        for col, keywords in tech_keywords.items():
            pattern = "|".join(keywords)
            df[col] = full_text.str.contains(pattern, regex=True).astype(int)

        # T·ªïng h·ª£p th√†nh Tech Score (0 -> 10)
        df['tech_score'] = (
            df['has_sunroof'] * 1.5 + 
            df['has_adas'] * 2.0 + 
            df['has_360'] * 1.5 + 
            df['has_leather'] * 1.0 + 
            df['has_smartkey'] * 1.0 +
            df['has_cruise'] * 1.0
        )

        # ======================================================================
        # 5. PH√ÇN KH√öC & LOGIC NGHI·ªÜP V·ª§ (BUSINESS LOGIC)
        # ======================================================================
        
        # Price Code (Ph√¢n kh√∫c gi√° chi ti·∫øt h∆°n cho VN)
        # 1: <400tr (Xe c·ªè)
        # 2: 400-700tr (Ph·ªï th√¥ng)
        # 3: 700-1.2 t·ª∑ (Trung c·∫•p/SUV C)
        # 4: 1.2-2.5 t·ª∑ (C·∫≠n sang/Sang nh·ªè)
        # 5: >2.5 t·ª∑ (Xe sang/Si√™u sang)
        def get_price_code(p):
            if p < 400_000_000: return 1
            if p < 700_000_000: return 2
            if p < 1_200_000_000: return 3
            if p < 2_500_000_000: return 4
            return 5
        df['price_code'] = df['price'].apply(get_price_code)

        # C·ªù ph√¢n lo·∫°i (D√πng cho Hard Filter v√† Persona Generator)
        df['is_family'] = ((df['n_seats'] >= 5) & (df['car_type'].isin(['suv', 'mpv', 'sedan']))).astype(int)
        df['is_service'] = ((df['price_code'] <= 2) & (df['fuel_category'].isin(['Diesel', 'Petrol'])) & (df['n_seats'] >= 4)).astype(int)
        df['is_luxury'] = ((df['price_code'] >= 4) | (df['make'].isin(['Mercedes-Benz', 'Bmw', 'Audi', 'Lexus', 'Porsche', 'Land-Rover', 'Volvo']))).astype(int)
        df['is_sport'] = ((df['power'] > 250) | (df['car_type'] == 'sport')).astype(int)
        df['is_green'] = (df['fuel_category'].isin(['Electric', 'Hybrid'])).astype(int)

        print(f"   -> ƒê√£ x·ª≠ l√Ω xong {len(df)} d√≤ng d·ªØ li·ªáu xe.")
        print(f"   -> C√°c c·ªôt m·ªõi: car_type, tech_score, is_green, fuel_category...")
        
        return df
# ==============================================================================
# 2. SINH D·ªÆ LI·ªÜU GI·∫¢ L·∫¨P (PERSONA GENERATOR)
# ==============================================================================
class PersonaGenerator:
    """
    Sinh d·ªØ li·ªáu User gi·∫£ l·∫≠p b·∫±ng c√°ch d√πng LLM (Gemma-3) ƒë√≥ng vai ng∆∞·ªùi d√πng th·∫≠t.
    Chi·∫øn l∆∞·ª£c: "Prototype & Clone" (T·∫°o m·∫´u b·∫±ng AI -> Nh√¢n b·∫£n b·∫±ng To√°n h·ªçc).
    """
    def __init__(self, df_cars, num_users=1000):
        self.df_cars = df_cars
        self.num_users = num_users
        
        # Kh·ªüi t·∫°o Client Gemini
        try:
            self.client = genai.Client(api_key=GEMINI_API_KEY)
            self.model_name = "gemini-2.5-flash-lite" # Ho·∫∑c "gemini-2.5-flash" n·∫øu account b·∫°n c√≥ quy·ªÅn
        except Exception as e:
            print(f"‚ùå L·ªói kh·ªüi t·∫°o Gemini: {e}. Vui l√≤ng ki·ªÉm tra API Key.")
            self.client = None

    def _get_llm_ratings_for_persona(self, persona_name, persona_desc, car_samples):
        """
        G·ª≠i danh s√°ch xe cho LLM v√† y√™u c·∫ßu ch·∫•m ƒëi·ªÉm theo vai (Persona)
        """
        print(f"   ü§ñ AI ƒëang ƒë√≥ng vai '{persona_name}' ƒë·ªÉ ch·∫•m ƒëi·ªÉm xe...")
        
        # T·∫°o text m√¥ t·∫£ danh s√°ch xe r√∫t g·ªçn ƒë·ªÉ ti·∫øt ki·ªám token
        cars_text = ""
        for _, row in car_samples.iterrows():
            # Gom th√¥ng tin quan tr·ªçng ƒë·ªÉ AI ƒë√°nh gi√°
            cars_text += (f"- ID: {row['id']} | Xe: {row['name']} | H√£ng: {row['brand']} | "
                          f"Gi√°: {row['price']:,} VNƒê | Lo·∫°i: {row['seats']} ch·ªó, {row['fuelType']}, {row.get('power', 100)}HP\n")

        prompt = f"""
        B·∫°n h√£y nh·∫≠p vai m·ªôt ng∆∞·ªùi d√πng √¥ t√¥ v·ªõi h·ªì s∆° sau:
        "{persona_desc}"

        D∆∞·ªõi ƒë√¢y l√† danh s√°ch c√°c m·∫´u xe √¥ t√¥ th·ª±c t·∫ø:
        {cars_text}

        Nhi·ªám v·ª•:
        1. D·ª±a tr√™n t√≠nh c√°ch v√† nhu c·∫ßu c·ªßa b·∫°n, h√£y ch·∫•m ƒëi·ªÉm t·ª´ng chi·∫øc xe tr√™n thang ƒëi·ªÉm t·ª´ 1.0 ƒë·∫øn 5.0.
        2. H√£y ch·∫•m ƒëi·ªÉm c√¥ng t√¢m d·ª±a tr√™n ki·∫øn th·ª©c th·ª±c t·∫ø (V√≠ d·ª•: Xe sang th√¨ ƒë·∫Øt nh∆∞ng s∆∞·ªõng, xe c·ªè th√¨ b·ªÅn nh∆∞ng ·ªìn, xe ƒëi·ªán th√¨ hi·ªán ƒë·∫°i...).
        3. TR·∫¢ V·ªÄ K·∫æT QU·∫¢ D·∫†NG JSON THU·∫¶N (Array of Objects), kh√¥ng gi·∫£i th√≠ch g√¨ th√™m.
        
        Format m·∫´u:
        [
            {{"id": "ID_XE_1", "rating": 4.5}},
            {{"id": "ID_XE_2", "rating": 2.0}}
        ]
        """

        try:
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(temperature=0.7) # Temp cao ch√∫t cho s√°ng t·∫°o
            )
            # Clean JSON string (ph√≤ng tr∆∞·ªùng h·ª£p LLM th√™m markdown)
            json_str = response.text.replace('```json', '').replace('```', '').strip()
            return json.loads(json_str)
        except Exception as e:
            print(f"   ‚ö†Ô∏è L·ªói khi g·ªçi AI cho {persona_name}: {e}")
            return []


    def generate_ratings(self):
        # N·∫øu kh√¥ng c√≥ API Key ho·∫∑c Client l·ªói -> Fallback v·ªÅ logic if-else c≈© (Code an to√†n)
        if not self.client:
            print("‚ö†Ô∏è Kh√¥ng c√≥ k·∫øt n·ªëi AI. Chuy·ªÉn v·ªÅ ch·∫ø ƒë·ªô sinh d·ªØ li·ªáu th·ªß c√¥ng (Rule-based).")
            return self._generate_ratings_fallback()

        print("2. [Data] ƒêang sinh Ratings th√¥ng minh b·∫±ng AI (Gemma-2/Flash)...")
        
        all_ratings = []
        
        # 1. Ch·ªçn m·∫´u xe (Sampling)
        # Kh√¥ng th·ªÉ g·ª≠i c·∫£ 1000 xe cho AI (t·ªën ti·ªÅn/token). 
        # Ta ch·ªçn 50 xe ti√™u bi·ªÉu ƒë·∫°i di·ªán cho c√°c ph√¢n kh√∫c.
        if len(self.df_cars) > 50:
            sample_cars = self.df_cars.sample(n=50, random_state=42)
        else:
            sample_cars = self.df_cars

        # 2. ƒê·ªãnh nghƒ©a Persona chi ti·∫øt (Prompt Engineering)
        personas = {
            'Student': "T√¥i l√† sinh vi√™n m·ªõi ra tr∆∞·ªùng, thu nh·∫≠p th·∫•p. T√¥i c·∫ßn xe gi√° r·∫ª, ti·∫øt ki·ªám xƒÉng, b·ªÅn b·ªâ, √≠t h·ªèng v·∫∑t (nh∆∞ Vios, Morning). T√¥i gh√©t xe sang v√¨ nu√¥i t·ªën k√©m.",
            'Family': "T√¥i l√† ng∆∞·ªùi ƒë√†n √¥ng c·ªßa gia ƒë√¨nh. T√¥i ∆∞u ti√™n xe r·ªông r√£i (5-7 ch·ªó), an to√†n, g·∫ßm cao (SUV/MPV) ƒë·ªÉ ch·ªü v·ª£ con ƒëi ch∆°i. Gi√° c·∫£ h·ª£p l√Ω l√† ƒë∆∞·ª£c.",
            'Boss': "T√¥i l√† doanh nh√¢n th√†nh ƒë·∫°t. T√¥i c·∫ßn xe sang tr·ªçng, th∆∞∆°ng hi·ªáu l·ªõn (Mercedes, BMW, Lexus, Porsche) ƒë·ªÉ th·ªÉ hi·ªán ƒë·∫≥ng c·∫•p. Gi√° c·∫£ kh√¥ng quan tr·ªçng, mi·ªÖn l√† ti·ªán nghi v√† √™m √°i.",
            'Racer': "T√¥i ƒëam m√™ t·ªëc ƒë·ªô v√† c√¥ng ngh·ªá. T√¥i th√≠ch xe c√≥ ƒë·ªông c∆° m·∫°nh m·∫Ω (m√£ l·ª±c cao), thi·∫øt k·∫ø th·ªÉ thao ho·∫∑c xe ƒëi·ªán c√¥ng ngh·ªá cao. T√¥i kh√¥ng th√≠ch xe y·∫øu ·ªõt."
        }

        # 3. V√≤ng l·∫∑p sinh d·ªØ li·ªáu
        users_per_persona = self.num_users // len(personas) # Chia ƒë·ªÅu user cho m·ªói nh√≥m

        for p_name, p_desc in personas.items():
            # A. L·∫•y ƒëi·ªÉm g·ªëc t·ª´ AI (Prototype Ratings)
            base_ratings = self._get_llm_ratings_for_persona(p_name, p_desc, sample_cars)
            
            if not base_ratings: continue # Skip n·∫øu l·ªói

            # B. Nh√¢n b·∫£n ra nhi·ªÅu User (Cloning with Noise)
            print(f"   -> ƒêang nh√¢n b·∫£n {users_per_persona} user cho nh√≥m {p_name}...")
            
            for _ in range(users_per_persona):
                # T·∫°o ID user ng·∫´u nhi√™n
                user_id = np.random.randint(100000, 999999)
                
                for item in base_ratings:
                    car_id = str(item.get('id'))
                    base_score = float(item.get('rating', 3.0))
                    
                    # TH√äM NHI·ªÑU (Noise): ƒê·ªÉ c√°c user kh√¥ng gi·ªëng nhau 100%
                    # Normal distribution: mean=0, std=0.4 (dao ƒë·ªông kho·∫£ng +/- 0.8 ƒëi·ªÉm)
                    noise = np.random.normal(0, 0.4)
                    final_score = np.clip(base_score + noise, 1, 5)
                    
                    # Random drop: User kh√¥ng nh·∫•t thi·∫øt ph·∫£i rate h·∫øt t·∫•t c·∫£ xe m·∫´u
                    # Gi·∫£ s·ª≠ user ch·ªâ rate 70% s·ªë xe m·∫´u
                    if np.random.rand() < 0.7:
                        all_ratings.append({
                            'user_id': user_id,
                            'car_id': car_id,
                            'rating': round(final_score, 1), # L√†m tr√≤n 1 s·ªë l·∫ª
                            'persona': p_name
                        })
            
            # Ngh·ªâ 1 ch√∫t ƒë·ªÉ tr√°nh hit rate limit c·ªßa Google
            time.sleep(2)

        df_ratings = pd.DataFrame(all_ratings)
        print(f"‚úÖ ƒê√£ sinh xong {len(df_ratings)} ratings t·ª´ AI.")
        return df_ratings

    def _generate_ratings_fallback(self):
        print("2. [Data] ƒêang sinh Ratings gi·∫£ l·∫≠p theo Persona (Student, Family, Boss)...")
        ratings = []
        
        # ƒê·ªãnh nghƒ©a c√°c nh√≥m ng∆∞·ªùi d√πng
        personas = ['Student', 'Family', 'Boss', 'Racer']
        
        for uid in range(self.num_users):
            p = np.random.choice(personas)
            
            # M·ªói user ƒë√°nh gi√° ng·∫´u nhi√™n 15-20 xe
            sample_cars = self.df_cars.sample(n=np.random.randint(15, 20))
            
            for _, car in sample_cars.iterrows():
                base_score = 3.0
                
                # --- LOGIC GI·∫¢ L·∫¨P S·ªû TH√çCH ---
                if p == 'Student': # Th√≠ch r·∫ª, gh√©t ƒë·∫Øt
                    if car['is_cheap']: base_score += 2.0
                    if car['is_luxury']: base_score -= 1.0
                    if car['mileage'] > 200000: base_score -= 0.5
                
                elif p == 'Family': # Th√≠ch r·ªông, an to√†n
                    if car['is_family']: base_score += 2.0
                    if car['car_type'] in ['coupe', 'convertible']: base_score -= 1.0
                    if car['year'] > 2016: base_score += 0.5
                
                elif p == 'Boss': # Th√≠ch sang, gh√©t r·∫ª
                    if car['is_luxury']: base_score += 2.0
                    if car['is_cheap']: base_score -= 1.0
                    if car['make'] in ['Mercedes-Benz', 'Bmw', 'Audi', 'Lexus']: base_score += 1.0
                
                elif p == 'Racer': # Th√≠ch m·∫°nh
                    if car['power'] > 150: base_score += 2.0
                    if car['car_type'] in ['coupe', 'convertible']: base_score += 1.0

                # Th√™m nhi·ªÖu (Noise) ƒë·ªÉ d·ªØ li·ªáu t·ª± nhi√™n h∆°n
                final_score = np.clip(base_score + np.random.uniform(-0.5, 0.5), 1, 5)
                
                ratings.append({
                    'user_id': uid, 
                    'car_id': car['id'], 
                    'rating': final_score, 
                    'persona': p
                })
        
        return pd.DataFrame(ratings)
# ==============================================================================
# 3. MODEL: TWO-TOWER NEURAL NETWORK (PYTORCH)
# ==============================================================================
class AdvancedTwoTowerNet(nn.Module):
    def __init__(self, 
                 n_users, n_personas, 
                 n_items, n_brands, n_car_types, 
                 embedding_dim=32):
        super(AdvancedTwoTowerNet, self).__init__()
        
        # --- USER TOWER ---
        # 1. Embeddings
        self.user_emb = nn.Embedding(n_users, embedding_dim)
        self.persona_emb = nn.Embedding(n_personas, 8) # Persona √≠t n√™n dim nh·ªè
        
        # 2. Dense Layers (MLP)
        # Input size = User Emb + Persona Emb
        self.user_layers = nn.Sequential(
            nn.Linear(embedding_dim + 8, 128),
            nn.BatchNorm1d(128), # Gi√∫p train ·ªïn ƒë·ªãnh h∆°n
            nn.ReLU(),
            nn.Dropout(0.3),     # Ch·ªëng overfitting
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32)    # Output vector size 32
        )
        
        # --- ITEM TOWER (XE) ---
        # 1. Embeddings (Cho d·ªØ li·ªáu ph√¢n lo·∫°i)
        self.item_emb = nn.Embedding(n_items, embedding_dim)
        self.brand_emb = nn.Embedding(n_brands, 16)
        self.type_emb = nn.Embedding(n_car_types, 8)
        
        # 2. Feature Transformation (Cho d·ªØ li·ªáu s·ªë: Price, Year, Power...)
        # Input: 5 ch·ªâ s·ªë s·ªë h·ªçc (Price norm, Year norm, Power norm, Seats norm, Tech Score)
        self.numeric_trans = nn.Linear(5, 16) 
        
        # 3. Dense Layers (MLP)
        # Input size = Item(32) + Brand(16) + Type(8) + Numeric(16) = 72
        self.item_layers = nn.Sequential(
            nn.Linear(embedding_dim + 16 + 8 + 16, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32)    # Output vector size 32 (Ph·∫£i kh·ªõp User Tower)
        )

    def forward(self, user_inputs, item_inputs):
        """
        user_inputs: [u_idx, persona_idx]
        item_inputs: [i_idx, brand_idx, type_idx, price, year, power, seats, tech]
        """
        
        # --- USER TOWER FORWARD ---
        u_idx = user_inputs[:, 0].long()
        p_idx = user_inputs[:, 1].long()
        
        u_vec = self.user_emb(u_idx)
        p_vec = self.persona_emb(p_idx)
        
        # N·ªëi vector (User ID + Persona)
        user_combined = torch.cat([u_vec, p_vec], dim=1)
        user_rep = self.user_layers(user_combined)
        
        # --- ITEM TOWER FORWARD ---
        i_idx = item_inputs[:, 0].long()
        b_idx = item_inputs[:, 1].long()
        t_idx = item_inputs[:, 2].long()
        # C√°c ch·ªâ s·ªë s·ªë h·ªçc (Price, Year...)
        numerics = item_inputs[:, 3:].float() 
        
        i_vec = self.item_emb(i_idx)
        b_vec = self.brand_emb(b_idx)
        t_vec = self.type_emb(t_idx)
        n_vec = F.relu(self.numeric_trans(numerics)) # Transform s·ªë h·ªçc
        
        # N·ªëi t·∫•t c·∫£ ƒë·∫∑c tr∆∞ng xe l·∫°i
        item_combined = torch.cat([i_vec, b_vec, t_vec, n_vec], dim=1)
        item_rep = self.item_layers(item_combined)
        
        # --- OUTPUT: DOT PRODUCT ---
        # T√≠nh t∆∞∆°ng ƒë·ªìng gi·ªØa Vector User t·ªïng h·ª£p v√† Vector Xe t·ªïng h·ª£p
        return (user_rep * item_rep).sum(dim=1)

# ==============================================================================
# 4. H·ªÜ TH·ªêNG G·ª¢I √ù CH√çNH (MAIN CLASS)
# ==============================================================================
class CarRecommendationSystem:
    def __init__(self, csv_path="D:\\Download\\learningdocument\\Khoa h·ªçc d·ªØ li·ªáu\\cuoiki\\KHDL\\scraped_cars.csv"):
        self.cp_dir = "checkpoints"
        if not os.path.exists(self.cp_dir): os.makedirs(self.cp_dir)

        # 1. Load & Process Data
        self.processor = CarDataProcessor(csv_path)
        self.df_cars = self.processor.process()

        # 2. Prepare Ratings & Encoders
        self._prepare_data()
        # --- N·∫†P FEEDBACK ---
        self.load_feedback_data() 
        # 3. Train or Load Models
        self._load_or_train_models()

        # 4. Build Item-Item Similarity Matrix (Slide Knowledge)
        self._build_item_similarity()

    def _prepare_data(self):
        path_ratings = f"{self.cp_dir}/ratings_gen.csv"
        
        if os.path.exists(path_ratings):
            print("2. [Data] Load ratings ƒë√£ l∆∞u t·ª´ cache...")
            self.df_ratings = pd.read_csv(path_ratings)
        else:
            gen = PersonaGenerator(self.df_cars)
            self.df_ratings = gen.generate_ratings()
            self.df_ratings.to_csv(path_ratings, index=False)
        # √âp c·∫£ 2 v·ªÅ string ƒë·ªÉ tr√°nh l·ªói "object and int64"
        self.df_ratings['car_id'] = self.df_ratings['car_id'].astype(str)
        self.df_cars['id'] = self.df_cars['id'].astype(str)
        # Encode ID sang s·ªë nguy√™n (0, 1, 2...) ƒë·ªÉ ƒë∆∞a v√†o Neural Net
        self.u_enc = LabelEncoder()
        self.i_enc = LabelEncoder()
        
        self.df_ratings['u_idx'] = self.u_enc.fit_transform(self.df_ratings['user_id'])
        # Fit tr√™n to√†n b·ªô xe ƒë·ªÉ tr√°nh l·ªói xe m·ªõi
        self.i_enc.fit(self.df_cars['id'])
        try:
            self.df_ratings['i_idx'] = self.i_enc.transform(self.df_ratings['car_id'])
        except ValueError:
            # Fallback ph√≤ng tr∆∞·ªùng h·ª£p rating ch·ª©a xe ƒë√£ b·ªã x√≥a kh·ªèi file csv g·ªëc
            # L·ªçc b·ªè c√°c rating c·ªßa xe kh√¥ng t·ªìn t·∫°i
            valid_cars = set(self.df_cars['id'])
            self.df_ratings = self.df_ratings[self.df_ratings['car_id'].isin(valid_cars)]
            self.df_ratings['i_idx'] = self.i_enc.transform(self.df_ratings['car_id'])
        # 2. Encoders cho Feature M·ªõi (Brand, Type, Persona)
        self.p_enc = LabelEncoder() # Persona
        self.b_enc = LabelEncoder() # Brand (Make)
        self.t_enc = LabelEncoder() # Car Type
        
        self.df_ratings['p_idx'] = self.p_enc.fit_transform(self.df_ratings['persona'])
        self.df_cars['b_idx'] = self.b_enc.fit_transform(self.df_cars['make'])
        self.df_cars['t_idx'] = self.t_enc.fit_transform(self.df_cars['car_type'])
        
        # 3. Chu·∫©n h√≥a d·ªØ li·ªáu s·ªë (MinMax Scaling th·ªß c√¥ng ƒë·ªÉ ƒë∆∞a v·ªÅ 0-1)
        # Price, Year, Power, Seats, TechScore
        cars = self.df_cars
        self.df_cars['norm_price'] = np.log1p(cars['price']) / np.log1p(cars['price'].max()) # Log ƒë·ªÉ gi·∫£m ch√™nh l·ªách gi√°
        self.df_cars['norm_year'] = (cars['year'] - 1990) / (2025 - 1990)
        self.df_cars['norm_power'] = cars['power'] / cars['power'].max()
        self.df_cars['norm_seats'] = cars['n_seats'] / 16.0
        self.df_cars['norm_tech'] = cars.get('tech_score', 0) / 10.0 # N·∫øu ch∆∞a c√≥ tech_score th√¨ = 0

        # L∆∞u s·ªë l∆∞·ª£ng classes ƒë·ªÉ init model
        self.dims = {
            'n_users': len(self.u_enc.classes_),
            'n_personas': len(self.p_enc.classes_),
            'n_items': len(self.i_enc.classes_),
            'n_brands': len(self.b_enc.classes_),
            'n_types': len(self.t_enc.classes_)
        }
        
        # Merge th√¥ng tin xe v√†o b·∫£ng ratings ƒë·ªÉ l√∫c train c√≥ d·ªØ li·ªáu
        # Ch·ªâ l·∫•y c√°c c·ªôt c·∫ßn thi·∫øt
        car_features = self.df_cars[['id', 'b_idx', 't_idx', 'norm_price', 'norm_year', 'norm_power', 'norm_seats', 'norm_tech']]
        self.train_df = pd.merge(self.df_ratings, car_features, left_on='car_id', right_on='id', how='left')

    def load_feedback_data(self):
            """
            [B·ªî SUNG] ƒê·ªçc d·ªØ li·ªáu feedback th·ª±c t·∫ø t·ª´ user_interactions_log.csv
            ƒë·ªÉ g·ªôp v√†o d·ªØ li·ªáu training g·ªëc.
            """
            log_path = "user_interactions_log.csv"
            if not os.path.exists(log_path):
                return

            try:
                # ƒê·ªçc log
                df_log = pd.read_csv(log_path)
                
                # Ch·ªâ l·∫•y c√°c h√†nh ƒë·ªông c√≥ ƒëi·ªÉm s·ªë (like, contact, view...)
                df_feedback = df_log[['user_id', 'car_id', 'implied_rating']].rename(columns={'implied_rating': 'rating'})
                
                # G√°n persona m·∫∑c ƒë·ªãnh (v√¨ log ch∆∞a c√≥ persona, ta coi l√† 'Mixed')
                df_feedback['persona'] = 'Mixed' 
                
                # √âp ki·ªÉu d·ªØ li·ªáu cho kh·ªõp v·ªõi b·∫£ng train g·ªëc
                df_feedback['user_id'] = df_feedback['user_id'].astype(str) # ID user th·∫≠t th∆∞·ªùng l√† string (uuid)
                df_feedback['car_id'] = df_feedback['car_id'].astype(str)
                
                print(f"4. [Feedback] ƒê√£ n·∫°p th√™m {len(df_feedback)} t∆∞∆°ng t√°c th·ª±c t·∫ø v√†o b·ªô nh·ªõ.")
                
                # G·ªôp v√†o df_ratings hi·ªán t·∫°i (ch·ªâ trong RAM, ch∆∞a l∆∞u ƒë√® file g·ªëc ƒë·ªÉ tr√°nh l·ªói)
                self.df_ratings = pd.concat([self.df_ratings, df_feedback], ignore_index=True)
                
                
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói ƒë·ªçc feedback log: {e}")

    def _load_or_train_models(self):
        path_svd = f"{self.cp_dir}/svd.pkl"
        path_torch = f"{self.cp_dir}/twotower.pth"

        # Check xem ƒë√£ train ch∆∞a
        if os.path.exists(path_svd) and os.path.exists(path_torch):
            print("3. [Model] ‚úÖ Load model ƒë√£ train t·ª´ checkpoint.")
            _, self.svd = dump.load(path_svd)
            self.torch_model = AdvancedTwoTowerNet(
                self.dims['n_users'], 
                self.dims['n_personas'],
                self.dims['n_items'], 
                self.dims['n_brands'], 
                self.dims['n_types']
            ).to(device)
            self.torch_model.load_state_dict(torch.load(path_torch, map_location=device))
            self.torch_model.eval()
            
            # ƒê√°nh gi√° l·∫°i nhanh
            self.evaluate_model()
        else:
            print("3. [Model] ‚ö†Ô∏è Ch∆∞a c√≥ model. B·∫Øt ƒë·∫ßu Train m·ªõi...")
            self._train_models(path_svd, path_torch)

    def _train_models(self, path_svd, path_torch):
        # A. Train SVD (Matrix Factorization)
        print("   -> Training SVD...")
        reader = Reader(rating_scale=(1, 5))
        data = SurpriseDataset.load_from_df(self.df_ratings[['user_id', 'car_id', 'rating']], reader)
        trainset = data.build_full_trainset()
        self.svd = SVD(n_factors=50, n_epochs=20, lr_all=0.005, reg_all=0.02)
        self.svd.fit(trainset)
        dump.dump(path_svd, algo=self.svd)

        # --- TRAIN TWO TOWER ---
        print("   -> Training Advanced Two-Tower Neural Network...")
        
        # Kh·ªüi t·∫°o model v·ªõi ƒë·∫ßy ƒë·ªß tham s·ªë k√≠ch th∆∞·ªõc
        self.torch_model = AdvancedTwoTowerNet(
            self.dims['n_users'], self.dims['n_personas'],
            self.dims['n_items'], self.dims['n_brands'], self.dims['n_types']
        ).to(device)
        self.torch_model.train()
        
        # Chu·∫©n b·ªã Tensor Input
        # User Input: [u_idx, p_idx]
        user_feats = self.train_df[['u_idx', 'p_idx']].values
        
        self.train_df['i_idx_mapped'] = self.i_enc.transform(self.train_df['car_id'])
        
        item_cols = ['i_idx_mapped', 'b_idx', 't_idx', 'norm_price', 'norm_year', 'norm_power', 'norm_seats', 'norm_tech']
        item_feats = self.train_df[item_cols].values
        
        targets = self.train_df['rating'].values

        # T·∫°o Tensor
        u_tensor = torch.tensor(user_feats, dtype=torch.float32) # S·∫Ω convert long b√™n trong model
        i_tensor = torch.tensor(item_feats, dtype=torch.float32)
        r_tensor = torch.tensor(targets, dtype=torch.float32)
        
        dataset = torch.utils.data.TensorDataset(u_tensor, i_tensor, r_tensor)
        loader = DataLoader(dataset, batch_size=64, shuffle=True)
        
        optimizer = optim.Adam(self.torch_model.parameters(), lr=0.001)
        criterion = nn.MSELoss()

        for epoch in range(20): # Train 20 epochs
            total_loss = 0
            for u_batch, i_batch, r_batch in loader:
                u_batch, i_batch, r_batch = u_batch.to(device), i_batch.to(device), r_batch.to(device)
                optimizer.zero_grad()
                preds = self.torch_model(u_batch, i_batch)
                loss = criterion(preds, r_batch)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            print(f"      Epoch {epoch+1}/20 - Loss: {total_loss/len(loader):.4f}")

        torch.save(self.torch_model.state_dict(), path_torch)
        print("   -> Train xong. ƒê√£ l∆∞u model.")
        
        # C. ƒê√°nh gi√° ngay sau khi train
        self.evaluate_model()
    def retrain_model(self):
        """
        [B·ªî SUNG] H√†m k√≠ch ho·∫°t hu·∫•n luy·ªán l·∫°i model ngay l·∫≠p t·ª©c.
        Quy tr√¨nh:
        1. ƒê·ªçc l·∫°i file log feedback.
        2. G·ªôp v√†o d·ªØ li·ªáu c≈©.
        3. Train l·∫°i SVD v√† Neural Network.
        4. C·∫≠p nh·∫≠t model n√≥ng trong RAM.
        """
        print("üîÑ [System] B·∫Øt ƒë·∫ßu quy tr√¨nh Retrain...")
        
        # 1. N·∫°p d·ªØ li·ªáu m·ªõi nh·∫•t
        self.load_feedback_data()
        
        # 2. ƒê·ªãnh nghƒ©a l·∫°i ƒë∆∞·ªùng d·∫´n l∆∞u checkpoint
        path_svd = f"{self.cp_dir}/svd.pkl"
        path_torch = f"{self.cp_dir}/twotower.pth"
        
        # 3. G·ªçi h√†m train (H√†m n√†y s·∫Ω update self.svd v√† self.torch_model)
        self._train_models(path_svd, path_torch)
        
        # 4. Re-build similarity matrix (ƒë·ªÉ c·∫≠p nh·∫≠t t√≠nh nƒÉng xe t∆∞∆°ng t·ª±)
        self._build_item_similarity()
        
        print("‚úÖ [System] Retrain ho√†n t·∫•t! Model ƒë√£ c·∫≠p nh·∫≠t ki·∫øn th·ª©c m·ªõi.")
        return True
    def _build_item_similarity(self):
        """
        X√¢y d·ª±ng ma tr·∫≠n t∆∞∆°ng ƒë·ªìng gi·ªØa c√°c xe (Item-Item CF)
        Ki·∫øn th·ª©c t·ª´ Slide: D√πng Cosine Similarity tr√™n ma tr·∫≠n User-Item
        """
        print("4. [Sim] ƒêang t√≠nh to√°n ma tr·∫≠n Item-Item Similarity (Slide Knowledge)...")
        # T·∫°o ma tr·∫≠n th∆∞a: H√†ng = Xe, C·ªôt = User, Gi√° tr·ªã = Rating
        pivot = self.df_ratings.pivot_table(index='car_id', columns='user_id', values='rating').fillna(0)
        
        # T√≠nh Cosine Similarity gi·ªØa c√°c XE
        self.item_sim_matrix = cosine_similarity(pivot)
        
        # L∆∞u index ƒë·ªÉ tra c·ª©u ng∆∞·ª£c
        self.sim_car_ids = pivot.index.tolist()
        print("   -> ƒê√£ x√¢y d·ª±ng xong ma tr·∫≠n t∆∞∆°ng ƒë·ªìng.")

    def evaluate_model(self, k=5):
        """
        ƒê√°nh gi√° to√†n di·ªán m√¥ h√¨nh:
        1. Regression Metrics: RMSE, MAE (D·ª± ƒëo√°n ƒëi·ªÉm c√≥ chu·∫©n kh√¥ng?)
        2. Ranking Metrics: Precision@K, Recall@K (Top K xe g·ª£i √Ω c√≥ 'ch·∫•t' kh√¥ng?)
        3. Segment Analysis: ƒê√°nh gi√° ri√™ng t·ª´ng nh√≥m Persona.
        """
        print("\n" + "="*60)
        print("üìä B√ÅO C√ÅO ƒê√ÅNH GI√Å HI·ªÜU QU·∫¢ M√î H√åNH (ADVANCED METRICS)")
        print("="*60)
        
        # ƒê·∫£m b·∫£o c·ªôt i_idx t·ªìn t·∫°i (ph√≤ng h·ªù)
        if 'i_idx' not in self.df_ratings.columns:
             self.df_ratings['i_idx'] = self.i_enc.transform(self.df_ratings['car_id'])

        # 1. Chu·∫©n b·ªã d·ªØ li·ªáu Test (20%)
        # L·∫•y m·∫´u ng·∫´u nhi√™n
        test_set = self.df_ratings.sample(frac=0.2, random_state=42)

        # Merge th√™m th√¥ng tin xe (Brand, Type, Specs...) v√†o test_set
        # ƒê·ªÉ c√≥ ƒë·ªß d·ªØ li·ªáu ƒë·∫ßu v√†o cho Item Tower
        cols_to_merge = ['id', 'b_idx', 't_idx', 'norm_price', 'norm_year', 'norm_power', 'norm_seats', 'norm_tech']
        test_set = pd.merge(test_set, self.df_cars[cols_to_merge], left_on='car_id', right_on='id', how='left')

        y_true = []
        y_pred = []
        
        # Gom nh√≥m theo User ƒë·ªÉ t√≠nh Ranking Metrics
        user_est_true = defaultdict(list)
        
        # Gom nh√≥m theo Persona ƒë·ªÉ ƒë√°nh gi√° ph√¢n kh√∫c
        persona_metrics = defaultdict(lambda: {'true': [], 'pred': []})

        print(f"   -> ƒêang ki·ªÉm tra tr√™n {len(test_set)} m·∫´u test...")
        
        self.torch_model.eval()
        
        with torch.no_grad():
            for _, row in test_set.iterrows():
                try:
                    uid = int(row['u_idx'])
                    cid = int(row['i_idx'])
                    persona = row['persona']
                    real_rating = row['rating']
                    
                    # --- 1. SVD PREDICTION ---
                    svd_pred = self.svd.predict(row['user_id'], row['car_id']).est
                    
                    # --- 2. TWO-TOWER PREDICTION ---
                    # Chu·∫©n b·ªã User Input [u_idx, p_idx] -> Shape [1, 2]
                    u_vals = [uid, row['p_idx']]
                    u_t = torch.tensor([u_vals], dtype=torch.float32).to(device) 
                    
                    # Chu·∫©n b·ªã Item Input [i_idx, b_idx, t_idx, price, year, power, seats, tech] -> Shape [1, 8]
                    # Ph·∫£i l·∫•y ƒë√∫ng th·ª© t·ª± nh∆∞ l√∫c train
                    i_vals = [
                        cid, 
                        row['b_idx'], 
                        row['t_idx'], 
                        row['norm_price'], 
                        row['norm_year'], 
                        row['norm_power'], 
                        row['norm_seats'], 
                        row['norm_tech']
                    ]
                    i_t = torch.tensor([i_vals], dtype=torch.float32).to(device)
                    
                    # D·ª± ƒëo√°n
                    dl_pred = float(self.torch_model(u_t, i_t).cpu().item())
                    
                    # --- 3. COMBINE ---
                    final_pred = 0.4 * svd_pred + 0.6 * dl_pred
                    
                    # L∆∞u k·∫øt qu·∫£
                    y_true.append(real_rating)
                    y_pred.append(final_pred)
                    
                    # L∆∞u cho Ranking
                    user_est_true[row['user_id']].append((final_pred, real_rating))
                    
                    # L∆∞u cho Persona Analysis
                    persona_metrics[persona]['true'].append(real_rating)
                    persona_metrics[persona]['pred'].append(final_pred)
                    
                except Exception as e:
                    # B·ªè qua n·∫øu c√≥ l·ªói d·ªØ li·ªáu nh·ªè l·∫ª
                    continue

        # ======================================================================
        # A. ƒê√ÅNH GI√Å ƒê·ªò CH√çNH X√ÅC (REGRESSION)
        # ======================================================================
        rmse = sqrt(mean_squared_error(y_true, y_pred))
        mae = mean_absolute_error(y_true, y_pred)
        
        print(f"\n1Ô∏è‚É£  ƒê·ªò CH√çNH X√ÅC ƒêI·ªÇM S·ªê (ACCURACY):")
        print(f"   - RMSE (Sai s·ªë b√¨nh ph∆∞∆°ng trung b√¨nh): {rmse:.4f} ‚≠ê (Th·∫•p h∆°n 1.0 l√† T·ªët)")
        print(f"   - MAE  (Sai s·ªë tuy·ªát ƒë·ªëi trung b√¨nh)  : {mae:.4f} ‚≠ê (L·ªách trung b√¨nh bao nhi√™u ƒëi·ªÉm)")

        # ======================================================================
        # B. ƒê√ÅNH GI√Å X·∫æP H·∫†NG (RANKING - PRECISION@K)
        # ======================================================================
        # ƒê·ªãnh nghƒ©a: "Relevant" (Th√≠ch) l√† rating >= 4.0
        precisions = []
        recalls = []
        
        for uid, user_ratings in user_est_true.items():
            # S·∫Øp x·∫øp c√°c xe ƒë√£ test theo ƒëi·ªÉm d·ª± ƒëo√°n gi·∫£m d·∫ßn
            user_ratings.sort(key=lambda x: x[0], reverse=True)
            
            # L·∫•y Top K
            top_k_items = user_ratings[:k]
            
            # ƒê·∫øm s·ªë l∆∞·ª£ng xe th·ª±c s·ª± th√≠ch trong Top K (True Rating >= 4.0)
            n_rel_and_rec = sum(1 for (_, true_r) in top_k_items if true_r >= 4.0)
            
            # T·ªïng s·ªë xe th·ª±c s·ª± th√≠ch trong to√†n b·ªô test set c·ªßa user n√†y
            n_rel = sum(1 for (_, true_r) in user_ratings if true_r >= 4.0)
            
            # Precision@K
            precisions.append(n_rel_and_rec / k if k > 0 else 0)
            
            # Recall@K
            recalls.append(n_rel_and_rec / n_rel if n_rel > 0 else 0)
            
        p_at_k = sum(precisions) / len(precisions) if precisions else 0
        r_at_k = sum(recalls) / len(recalls) if recalls else 0
        
        print(f"\n2Ô∏è‚É£  CH·∫§T L∆Ø·ª¢NG G·ª¢I √ù (RANKING @{k}):")
        print(f"   - Precision@{k}: {p_at_k:.2%} (T·ªâ l·ªá xe user th√≠ch trong top {k})")
        print(f"   - Recall@{k}   : {r_at_k:.2%} (T·ªâ l·ªá t√¨m th·∫•y xe ngon trong kho)")

        # ======================================================================
        # C. PH√ÇN T√çCH THEO NH√ìM NG∆Ø·ªúI D√ôNG (SEGMENTATION)
        # ======================================================================
        print(f"\n3Ô∏è‚É£  HI·ªÜU NƒÇNG THEO PERSONA (SEGMENTATION):")
        print(f"   {'Persona':<15} | {'RMSE':<10} | {'MAE':<10} | {'Tr·∫°ng th√°i'}")
        print("-" * 55)
        
        for p, data in persona_metrics.items():
            if not data['true']: continue
            p_rmse = sqrt(mean_squared_error(data['true'], data['pred']))
            p_mae = mean_absolute_error(data['true'], data['pred'])
            
            status = "‚úÖ T·ªët" if p_rmse < 1.0 else ("‚ö†Ô∏è Kh√°" if p_rmse < 1.2 else "‚ùå K√©m")
            print(f"   {p:<15} | {p_rmse:.4f}     | {p_mae:.4f}     | {status}")
            
        print("="*60 + "\n")
        return rmse

    def recommend(self, profile_dict, top_k=5):
        """
        H√†m g·ª£i √Ω t·ªëi ∆∞u h√≥a cho d·ªØ li·ªáu th·ª±c t·∫ø (Scraped CSV).
        Quy tr√¨nh:
        1. L·ªçc c·ª©ng (Specific Brands -> Text Search -> Technical Specs -> Persona).
        2. C∆° ch·∫ø Fallback th√¥ng minh (N·∫øu l·ªçc h·∫øt xe -> N·ªõi l·ªèng ƒëi·ªÅu ki·ªán).
        3. Ch·∫•m ƒëi·ªÉm Hybrid (Vector Cosine + SVD Rating + Rule-based Boost).
        """
        
        # --- B∆Ø·ªöC 1: KH·ªûI T·∫†O B·ªò L·ªåC ---
        # Ta t·∫°o m·ªôt b·∫£n sao ƒë·ªÉ l·ªçc d·∫ßn
        filtered_df = self.df_cars.copy()
        
        # Flag ƒë·ªÉ bi·∫øt xem c√≥ ƒëang l·ªçc qu√° g·∫Øt kh√¥ng
        initial_count = len(filtered_df)
        
        # ----------------------------------------------------------------------
        # A. L·ªåC THEO H√ÉNG C·ª§ TH·ªÇ (∆Øu ti√™n cao nh·∫•t - D√†nh cho l·ªánh So s√°nh)
        # ----------------------------------------------------------------------
        if 'specific_brands' in profile_dict and profile_dict['specific_brands']:
            target_brands = [b.lower() for b in profile_dict['specific_brands']]
            # T√¨m t∆∞∆°ng ƒë·ªëi: V√≠ d·ª• user n√≥i "Merc" th√¨ v·∫´n ra "Mercedes-Benz"
            filtered_df = filtered_df[filtered_df['make'].str.lower().apply(
                lambda x: any(t in x for t in target_brands)
            )]

        # ----------------------------------------------------------------------
        # B. T√åM KI·∫æM T·ª™ KH√ìA (SEMANTIC SEARCH TH√î)
        # ----------------------------------------------------------------------
        if 'search_query' in profile_dict:
            q = profile_dict['search_query'].lower()
            
            # 1. T√¨m theo lo·∫°i xe
            if 'suv' in q or 'g·∫ßm cao' in q or '7 ch·ªó' in q:
                # T√¨m trong t√™n xe ho·∫∑c s·ªë ch·ªó
                filtered_df = filtered_df[
                    (filtered_df['name'].str.lower().str.contains('suv|cross|fortuner|everest|santa|sorento')) | 
                    (filtered_df['n_seats'] >= 7)
                ]
            elif 'sedan' in q:
                filtered_df = filtered_df[filtered_df['n_seats'] <= 5]
            elif 'b√°n t·∫£i' in q or 'pickup' in q:
                filtered_df = filtered_df[filtered_df['name'].str.lower().str.contains('ranger|triton|hilux|navara')]

            # 2. T√¨m theo t√≠nh nƒÉng (trong description ho·∫∑c features)
            # V√≠ d·ª•: "xe c√≥ c·ª≠a s·ªï tr·ªùi"
            keywords = ['c·ª≠a s·ªï tr·ªùi', 'camera 360', 'gh·∫ø da', 'turbo']
            for kw in keywords:
                if kw in q:
                    filtered_df = filtered_df[
                        filtered_df['features'].str.lower().str.contains(kw, na=False) |
                        filtered_df['description'].str.lower().str.contains(kw, na=False)
                    ]

        # ----------------------------------------------------------------------
        # C. L·ªåC K·ª∏ THU·∫¨T (TECHNICAL SPECS)
        # ----------------------------------------------------------------------
        # [B·ªî SUNG] L·ªçc theo gi√° tr·∫ßn t·ª´ c√¢u chat (n·∫øu c√≥)
        if 'max_price_override' in profile_dict and profile_dict['max_price_override'] > 0:
            max_p = profile_dict['max_price_override']
            # Cho ph√©p dung sai 10% (xe ƒë·∫Øt h∆°n x√≠u v·∫´n l·∫•y)
            filtered_df = filtered_df[filtered_df['price'] <= max_p * 1.1]
        # NƒÉm s·∫£n xu·∫•t
        if 'min_year' in profile_dict:
            filtered_df = filtered_df[filtered_df['year'] >= profile_dict['min_year']]
        
        # S·ª©c m·∫°nh ƒë·ªông c∆° (M√£ l·ª±c)
        if 'min_power' in profile_dict:
            filtered_df = filtered_df[filtered_df['power'] >= profile_dict['min_power']]
            
        # Nhi√™n li·ªáu (Map t·ª´ input User sang d·ªØ li·ªáu CSV 'XƒÉng'/'D·∫ßu'/'ƒêi·ªán')
        if 'fuel_type' in profile_dict:
            req = profile_dict['fuel_type'].lower()
            if req in ['xƒÉng', 'petrol']:
                filtered_df = filtered_df[filtered_df['fuelType'].str.lower() == 'xƒÉng']
            elif req in ['d·∫ßu', 'diesel']:
                filtered_df = filtered_df[filtered_df['fuelType'].str.lower() == 'd·∫ßu']
            elif req in ['ƒëi·ªán', 'electric', 'ev']:
                filtered_df = filtered_df[filtered_df['fuelType'].str.lower().isin(['ƒëi·ªán', 'hybrid'])]

        # ----------------------------------------------------------------------
        # D. L·ªåC THEO PERSONA (N·∫øu ch∆∞a b·ªã l·ªçc b·ªüi Brand/Query)
        # ----------------------------------------------------------------------
        # Ch·ªâ √°p d·ª•ng n·∫øu danh s√°ch c√≤n nhi·ªÅu xe (> 10) ƒë·ªÉ tr√°nh filtered_df b·ªã r·ªóng
        if len(filtered_df) > 10:
            persona = profile_dict.get('persona', 'Family')
            
            if persona == 'Student': # ∆Øu ti√™n xe r·∫ª
                filtered_df = filtered_df[filtered_df['price_code'] <= 2] 
            
            elif persona == 'Family': # ∆Øu ti√™n xe r·ªông, ƒë·ªùi kh√¥ng qu√° s√¢u
                filtered_df = filtered_df[filtered_df['is_family'] == 1]
                filtered_df = filtered_df[filtered_df['year'] >= 2015]
            
            elif persona == 'Boss': # ∆Øu ti√™n xe sang
                filtered_df = filtered_df[filtered_df['is_luxury'] == 1]
            
            elif persona == 'Racer': # ∆Øu ti√™n xe m·∫°nh
                filtered_df = filtered_df[filtered_df['power'] > 150]
        print(f"üîç [Filter Stats] Ban ƒë·∫ßu: {initial_count} xe -> Sau khi l·ªçc: {len(filtered_df)} xe")
        
        if len(filtered_df) == 0:
            print("   ‚ö†Ô∏è C·∫£nh b√°o: B·ªô l·ªçc qu√° ch·∫∑t, kh√¥ng c√≤n xe n√†o kh·ªõp!")
        elif len(filtered_df) < initial_count * 0.1:
            print("   ‚ö†Ô∏è C·∫£nh b√°o: ƒê√£ l·ªçc b·ªè h∆°n 90% d·ªØ li·ªáu, k·∫øt qu·∫£ c√≥ th·ªÉ b·ªã h·∫°n ch·∫ø.")

        # ----------------------------------------------------------------------
        # E. FEATURE MATCHING (L·ªçc m·ªÅm b·∫±ng t·ª´ kh√≥a)
        # ----------------------------------------------------------------------
        # N·∫øu user y√™u c·∫ßu t√≠nh nƒÉng c·ª• th·ªÉ (VD: C·ª≠a s·ªï tr·ªùi), ta ∆∞u ti√™n l·ªçc.
        # Nh∆∞ng n·∫øu l·ªçc xong c√≤n qu√° √≠t xe (<3), ta s·∫Ω b·ªè qua b∆∞·ªõc n√†y (Fallback).
        
        req_features = profile_dict.get('features', [])
        if req_features:
            temp_df = filtered_df.copy()
            # Map t·ª´ kh√≥a AI tr·∫£ v·ªÅ sang c·ªôt d·ªØ li·ªáu (ƒë√£ t·∫°o ·ªü b∆∞·ªõc clean data)
            feature_map = {
                'sunroof': 'has_sunroof',
                'adas': 'has_adas',
                '360_camera': 'has_360',
                'leather': 'has_leather',
                'smartkey': 'has_smartkey'
            }
            
            for req in req_features:
                col = feature_map.get(req)
                if col and col in temp_df.columns:
                    # L·ªçc xe c√≥ t√≠nh nƒÉng n√†y
                    temp_df = temp_df[temp_df[col] == 1]
            
            # Ch·ªâ √°p d·ª•ng n·∫øu c√≤n xe (tr√°nh tr·∫£ v·ªÅ r·ªóng)
            if len(temp_df) >= 3:
                filtered_df = temp_df
                print(f"‚ú® [Engine] ƒê√£ l·ªçc theo t√≠nh nƒÉng: {req_features}")
            else:
                print(f"‚ö†Ô∏è [Engine] Kh√¥ng t√¨m th·∫•y xe c√≥ {req_features}, b·ªè qua filter n√†y.")
        # ----------------------------------------------------------------------
        # E. C∆† CH·∫æ FALLBACK (C·ª®U C√ÅNH)
        # ----------------------------------------------------------------------
        # N·∫øu l·ªçc xong m√† c√≤n √≠t h∆°n 3 xe -> N·ªõi l·ªèng ƒëi·ªÅu ki·ªán
        if len(filtered_df) < 3:
            # Reset l·∫°i t·∫≠p l·ªçc (L·∫•y l·∫°i to√†n b·ªô xe kh·ªõp Brand/Query nh∆∞ng b·ªè qua NƒÉm/Specs)
            # Ho·∫∑c t·ªá nh·∫•t l√† l·∫•y to√†n b·ªô DB
            candidates_ids = self.df_cars['id'].tolist()
        else:
            candidates_ids = filtered_df['id'].tolist()

        # Sampling: N·∫øu c√≤n qu√° nhi·ªÅu xe (>200), l·∫•y ng·∫´u nhi√™n 200 ƒë·ªÉ t√≠nh to√°n cho nhanh
        if len(candidates_ids) > 200:
            candidates_ids = np.random.choice(candidates_ids, 200, replace=False)

        # ----------------------------------------------------------------------
        # F. SCORING ENGINE (CH·∫§M ƒêI·ªÇM)
        # ----------------------------------------------------------------------
        
        # 1. X√°c ƒë·ªãnh Proxy User
        persona = profile_dict.get('persona', 'Family')
        proxy_users = self.df_ratings[self.df_ratings['persona'] == persona]['user_id'].unique()
        user_id = proxy_users[0] if len(proxy_users) > 0 else self.df_ratings['user_id'].iloc[0]
        
        # 2. Chu·∫©n b·ªã Tensor
        u_idx = self.u_enc.transform([user_id])[0]
        try:
            c_idxs = self.i_enc.transform(candidates_ids)
        except:
            # Fallback n·∫øu g·∫∑p ID l·∫° (xe m·ªõi c√†o th√™m m√† ch∆∞a train)
            return self.df_cars[self.df_cars['id'].isin(candidates_ids)].head(top_k)

        # Chu·∫©n b·ªã User Features [u_idx, p_idx]
        p_idx = self.p_enc.transform([persona])[0]
        u_input = torch.tensor([[u_idx, p_idx]] * len(candidates_ids)).to(device) # Shape [N, 2]
        
        # Chu·∫©n b·ªã Item Features
        # L·∫•y th√¥ng tin c√°c xe candidates t·ª´ df_cars
        candidate_cars = self.df_cars[self.df_cars['id'].isin(candidates_ids)].set_index('id')
        candidate_cars = candidate_cars.reindex(candidates_ids) # ƒê·∫£m b·∫£o ƒë√∫ng th·ª© t·ª±
        
        # L·∫•y c√°c c·ªôt features
        c_idxs = self.i_enc.transform(candidates_ids)
        b_idxs = candidate_cars['b_idx'].values
        t_idxs = candidate_cars['t_idx'].values
        numerics = candidate_cars[['norm_price', 'norm_year', 'norm_power', 'norm_seats', 'norm_tech']].values
        
        # G·ªôp l·∫°i th√†nh tensor [N, 8]
        # [i_idx, b_idx, t_idx, price, year, power, seats, tech]
        i_data = np.column_stack([c_idxs, b_idxs, t_idxs, numerics])
        i_input = torch.tensor(i_data, dtype=torch.float32).to(device)
        
        self.torch_model.eval()
        with torch.no_grad():
            
            # Forward user part
            u_vec = self.torch_model.user_emb(u_input[:, 0].long())
            p_vec = self.torch_model.persona_emb(u_input[:, 1].long())
            user_rep = self.torch_model.user_layers(torch.cat([u_vec, p_vec], dim=1))
            
            # Forward item part
            i_vec = self.torch_model.item_emb(i_input[:, 0].long())
            b_vec = self.torch_model.brand_emb(i_input[:, 1].long())
            t_vec = self.torch_model.type_emb(i_input[:, 2].long())
            n_vec = F.relu(self.torch_model.numeric_trans(i_input[:, 3:].float()))
            item_rep = self.torch_model.item_layers(torch.cat([i_vec, b_vec, t_vec, n_vec], dim=1))
            
            # T√≠nh Cosine & Rating
            cosine_scores = F.cosine_similarity(user_rep, item_rep).cpu().numpy()
            
            # T√≠nh Dot Product -> Predicted Rating
            dl_ratings = (user_rep * item_rep).sum(dim=1).cpu().numpy()

        # L·∫•y danh s√°ch xe user ƒë√£ Like trong phi√™n n√†y - feedback
        liked_car_ids = profile_dict.get('liked_history', [])

        # 4. T·ªïng h·ª£p k·∫øt qu·∫£
        results = []
        for idx, car_id in enumerate(candidates_ids):
            # L·∫•y th√¥ng tin xe ƒë·ªÉ Boost ƒëi·ªÉm
            car_info = self.df_cars[self.df_cars['id'] == car_id].iloc[0]
            
            # ƒêi·ªÉm c∆° b·∫£n
            svd_rating = self.svd.predict(user_id, car_id).est
            dl_rating = float(dl_ratings[idx])
            final_rating = 0.4 * svd_rating + 0.6 * dl_rating
            
            # T√≠nh Match Score (%)
            raw_match = (cosine_scores[idx] + 1) / 2 # Normalize 0-1
            match_percent = raw_match * 100
            # --- [N√ÇNG C·∫§P] RULE-BASED BOOSTING ---
            
            # 1. Boost theo Hi·ªáu su·∫•t (N·∫øu user th√≠ch xe m·∫°nh)
            if profile_dict.get('high_performance', False):
                # M√£ l·ª±c > 180 l√† m·∫°nh
                if car_info['power'] > 180:
                    match_percent += 15
                    final_rating += 0.8
                # Turbo th∆∞·ªùng m·∫°nh
                if 'turbo' in str(car_info['engine']).lower():
                    match_percent += 5

            # 2. Boost theo T√¨nh tr·∫°ng (Xe l∆∞·ªõt)
            if profile_dict.get('car_condition') == 'like_new':
                # Xe d∆∞·ªõi 3 tu·ªïi v√† ODO th·∫•p
                if car_info['age'] <= 3 and car_info['mileage'] < 40000:
                    match_percent += 10
                    final_rating += 0.5
            # --- RULE-BASED BOOSTING (C·ªông ƒëi·ªÉm th∆∞·ªüng) ---
            # Th∆∞·ªüng cho xe ƒë·ªùi m·ªõi (> 2022)
            if car_info['year'] >= 2022: 
                match_percent += 5
                final_rating += 0.2
            
            # Th∆∞·ªüng n·∫øu ƒë√∫ng Brand y√™u th√≠ch (n·∫øu c√≥ trong profile)
            if 'preferredBrands' in profile_dict and profile_dict['preferredBrands']:
                fav_brands = [b.lower() for b in profile_dict['preferredBrands']]
                if str(car_info['make']).lower() in fav_brands:
                    match_percent += 10
                    final_rating += 0.5
            # --- REAL-TIME FEEDBACK BOOSTING ---
            # N·∫øu xe n√†y t∆∞∆°ng ƒë·ªìng v·ªõi xe user v·ª´a Like -> C·ªông ƒëi·ªÉm c·ª±c m·∫°nh
            if liked_car_ids:
                # Ki·ªÉm tra xem xe hi·ªán t·∫°i (car_id) c√≥ gi·ªëng xe ƒë√£ like kh√¥ng
                # D√πng ma tr·∫≠n item_sim_matrix ƒë√£ t√≠nh
                for liked_id in liked_car_ids:
                    if liked_id in self.sim_car_ids and car_id in self.sim_car_ids:
                        # L·∫•y index
                        idx_curr = self.sim_car_ids.index(car_id)
                        idx_liked = self.sim_car_ids.index(liked_id)
                        
                        # L·∫•y ƒë·ªô t∆∞∆°ng ƒë·ªìng (0 -> 1)
                        sim_score = self.item_sim_matrix[idx_curr][idx_liked]
                        
                        if sim_score > 0.6: # N·∫øu gi·ªëng > 60%
                            boost = sim_score * 15 # C·ªông t·ªëi ƒëa 15% match
                            match_percent += boost
                            final_rating += (sim_score * 1.0) # C·ªông t·ªëi ƒëa 1 ƒëi·ªÉm rating
                            # Break ƒë·ªÉ kh√¥ng c·ªông d·ªìn qu√° nhi·ªÅu n·∫øu like nhi·ªÅu xe gi·ªëng nhau
                            break 
            # ---------------------------------------------
            # Clip k·∫øt qu·∫£
            match_percent = min(99, int(match_percent))
            
            results.append({
                'id': car_id, 
                'score': final_rating, 
                'match_percent': match_percent
            })
            
        # S·∫Øp x·∫øp theo ƒëi·ªÉm cao nh·∫•t
        res_df = pd.DataFrame(results).sort_values('score', ascending=False).head(top_k)
        
        # Merge l·∫°i ƒë·ªÉ l·∫•y full th√¥ng tin xe
        final_df = pd.merge(res_df, self.df_cars, on='id')
        return final_df
    def _get_content_based_similar_cars(self, car_id, top_k=5):
        """
        FALLBACK: T√¨m xe t∆∞∆°ng t·ª± d·ª±a tr√™n th√¥ng s·ªë k·ªπ thu·∫≠t (d√πng khi ch∆∞a c√≥ rating).
        Logic: C√πng ph√¢n kh√∫c (Body Type) -> C√πng t·∫ßm gi√° -> C√πng h√£ng (∆∞u ti√™n).
        """
        # 1. L·∫•y th√¥ng tin xe g·ªëc
        try:
            # ƒê·∫£m b·∫£o ID l√† string ƒë·ªÉ so s√°nh
            car_id = str(car_id)
            target_car = self.df_cars[self.df_cars['id'] == car_id].iloc[0]
        except IndexError:
            return pd.DataFrame() # Xe kh√¥ng t·ªìn t·∫°i trong kho

        # 2. L·ªçc xe c√πng ki·ªÉu d√°ng (Body Type)
        # Gi·∫£ s·ª≠ ƒë√£ c√≥ c·ªôt 'car_type' t·ª´ h√†m process(), n·∫øu ch∆∞a th√¨ d√πng logic ƒë∆°n gi·∫£n
        target_type = target_car.get('car_type', '')
        
        # L·∫•y danh s√°ch ·ª©ng vi√™n (tr·ª´ ch√≠nh n√≥)
        candidates = self.df_cars[self.df_cars['id'] != car_id].copy()
        
        # T√≠nh ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng (Distance Metric)
        # C√¥ng th·ª©c: 
        # - C√πng Body Type: +40ƒë
        # - C√πng H√£ng: +20ƒë
        # - Ch√™nh l·ªách gi√°: T·ªëi ƒëa 40ƒë (c√†ng g·∫ßn c√†ng cao)
        
        def calculate_similarity(row):
            score = 0
            
            # 1. Body Type (Quan tr·ªçng nh·∫•t)
            if row.get('car_type') == target_type:
                score += 40
            
            # 2. Brand
            if row['make'] == target_car['make']:
                score += 20
                
            # 3. Price Similarity (Max 40 ƒëi·ªÉm)
            # T√≠nh % ch√™nh l·ªách gi√°. V√≠ d·ª• l·ªách 0% -> 40ƒë, l·ªách 50% -> 0ƒë
            try:
                price_diff = abs(row['price'] - target_car['price'])
                percent_diff = price_diff / (target_car['price'] + 1) # +1 tr√°nh chia 0
                price_score = max(0, 40 * (1 - percent_diff * 2)) # L·ªách 50% l√† h·∫øt ƒëi·ªÉm
                score += price_score
            except:
                pass
                
            # 4. Year Similarity (Bonus nh·∫π)
            year_diff = abs(row['year'] - target_car['year'])
            if year_diff <= 2: score += 5
            
            return score

        candidates['sim_score'] = candidates.apply(calculate_similarity, axis=1)
        
        # L·∫•y top K xe c√≥ ƒëi·ªÉm cao nh·∫•t
        top_candidates = candidates.sort_values('sim_score', ascending=False).head(top_k)
        
        print(f"   ‚ú® [Content-Based] T√¨m th·∫•y {len(top_candidates)} xe t∆∞∆°ng t·ª± theo th√¥ng s·ªë.")
        return top_candidates

    def get_similar_cars_item_based(self, car_id, top_k=3):
        """
        HYBRID SIMILARITY:
        1. Th·ª≠ t√¨m b·∫±ng Item-Item CF (H√†nh vi ng∆∞·ªùi d√πng - Ch√≠nh x√°c nh·∫•t).
        2. N·∫øu kh√¥ng c√≥ (xe m·ªõi), Fallback sang Content-Based (Th√¥ng s·ªë k·ªπ thu·∫≠t).
        """
        car_id = str(car_id)
        print(f"\nüîç T√¨m xe t∆∞∆°ng t·ª± cho xe ID: {car_id}")
        
        cf_results = pd.DataFrame()
        
        # --- C√ÅCH 1: COLLABORATIVE FILTERING (∆Øu ti√™n) ---
        if hasattr(self, 'sim_car_ids') and car_id in self.sim_car_ids:
            try:
                # L·∫•y index
                idx = self.sim_car_ids.index(car_id)
                # L·∫•y vector t∆∞∆°ng ƒë·ªìng
                sim_scores = self.item_sim_matrix[idx]
                # Sort l·∫•y index cao nh·∫•t (tr·ª´ ch√≠nh n√≥)
                top_indices = sim_scores.argsort()[-(top_k+1):-1][::-1]
                similar_ids = [self.sim_car_ids[i] for i in top_indices]
                
                cf_results = self.df_cars[self.df_cars['id'].isin(similar_ids)]
                print(f"   ‚úÖ [CF] T√¨m th·∫•y {len(cf_results)} xe d·ª±a tr√™n h√†nh vi ng∆∞·ªùi d√πng.")
            except Exception as e:
                print(f"   ‚ö†Ô∏è L·ªói CF: {e}")

        # --- C√ÅCH 2: CONTENT-BASED (Fallback ho·∫∑c B·ªï sung) ---
        # N·∫øu CF kh√¥ng tr·∫£ v·ªÅ ƒë·ªß s·ªë l∆∞·ª£ng xe (v√≠ d·ª• top_k=3 m√† CF ch·ªâ ra 0 ho·∫∑c 1 xe)
        # Ch√∫ng ta s·∫Ω t√¨m th√™m b·∫±ng Content-Based ƒë·ªÉ l·∫•p ƒë·∫ßy
        if len(cf_results) < top_k:
            needed = top_k - len(cf_results)
            print(f"   ‚ö†Ô∏è CF ch∆∞a ƒë·ªß (c√≥ {len(cf_results)}/{top_k}), t√¨m th√™m b·∫±ng Content-Based...")
            
            cb_results = self._get_content_based_similar_cars(car_id, top_k=needed + 5) # L·∫•y d∆∞ ra ƒë·ªÉ l·ªçc tr√πng
            
            # Lo·∫°i b·ªè xe ƒë√£ c√≥ trong CF
            if not cf_results.empty:
                cb_results = cb_results[~cb_results['id'].isin(cf_results['id'])]
            
            # G·ªôp l·∫°i: CF l√™n ƒë·∫ßu, Content-Based theo sau
            final_results = pd.concat([cf_results, cb_results.head(needed)])
            return final_results
            
        return cf_results

# --- MAIN TEST ---
if __name__ == "__main__":
    recsys = CarRecommendationSystem()
    
    # Test Evaluate
    recsys.evaluate_model()
    
    # Test Recommend cho Sinh vi√™n
    print("\n--- TEST RECOMMEND: STUDENT ---")
    profile = {'persona': 'Student'}
    print(recsys.recommend(profile))
    
    # Test Item-Item (Slide)
    print("\n--- TEST SIMILAR CARS (ITEM-ITEM) ---")
    sample_car = recsys.df_cars['id'].iloc[0]
    print(f"T√¨m xe gi·ªëng xe ID {sample_car}:")
    print(recsys.get_similar_cars_item_based(sample_car))